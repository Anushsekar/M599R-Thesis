{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This cell loads libraries, sets global configuration and experiment parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T15:56:59.075301Z",
     "iopub.status.busy": "2026-01-06T15:56:59.074986Z",
     "iopub.status.idle": "2026-01-06T15:57:09.266503Z",
     "shell.execute_reply": "2026-01-06T15:57:09.265984Z",
     "shell.execute_reply.started": "2026-01-06T15:56:59.075279Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    "    balanced_accuracy_score,\n",
    "    matthews_corrcoef,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "CSV_PATH = \"/kaggle/input/aci-iot-network-traffic-dataset-2023/ACI-IoT-2023-Payload.csv\"\n",
    "LABEL_COLUMN = \"label\"\n",
    "BENIGN_LABEL_NAME = \"Benign\"\n",
    "\n",
    "EXPERIMENT_CONFIG = {\n",
    "    \"sample_frac\": 0.2,           # increase later if time allows\n",
    "    \"val_size\": 0.2,              # fraction of temp set\n",
    "    \"test_size\": 0.2,             # fraction of full set\n",
    "    \"max_dbscan_samples\": 40_000,\n",
    "    \"dbscan_eps\": 1.5,\n",
    "    \"dbscan_min_samples\": 20,\n",
    "    \"pca_components_dbscan\": 3,\n",
    "    \"ae_epochs\": 20,\n",
    "    \"ae_batch_size\": 512,\n",
    "    \"common_ports\": [80, 443, 53, 22, 1883],\n",
    "}\n",
    "\n",
    "# Optional: force CPU for TensorFlow (reduces noisy CUDA init logs)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "KERAS_AVAILABLE = False\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Model\n",
    "    from tensorflow.keras.layers import Input, Dense\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    KERAS_AVAILABLE = True\n",
    "    print(\"[INFO] TensorFlow/Keras detected. Autoencoder will be available.\")\n",
    "except Exception as e:\n",
    "    print(\"[INFO] TensorFlow/Keras NOT available. Autoencoder will be skipped.\")\n",
    "    KERAS_AVAILABLE = False\n",
    "\n",
    "print(\"Imports & configuration ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loads the dataset and optionally samples it for faster experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T15:57:13.620227Z",
     "iopub.status.busy": "2026-01-06T15:57:13.619716Z",
     "iopub.status.idle": "2026-01-06T15:59:00.229477Z",
     "shell.execute_reply": "2026-01-06T15:59:00.228652Z",
     "shell.execute_reply.started": "2026-01-06T15:57:13.620205Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_aci_iot(csv_path: str, sample_frac: float = 1.0, random_state: int = RANDOM_SEED) -> pd.DataFrame:\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"Path not found: {csv_path}\")\n",
    "\n",
    "    print(f\"[INFO] Loading dataset from: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"[INFO] Original shape: {df.shape}\")\n",
    "\n",
    "    if 0 < sample_frac < 1.0:\n",
    "        df = df.sample(frac=sample_frac, random_state=random_state).reset_index(drop=True)\n",
    "        print(f\"[INFO] After downsampling (frac={sample_frac}): {df.shape}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "df = load_aci_iot(CSV_PATH, sample_frac=EXPERIMENT_CONFIG[\"sample_frac\"])\n",
    "print(\"\\n[INFO] First 5 rows:\")\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creates binary_label (Benign=0, Attack=1) for evaluation and builds engineered features.\n",
    "## Drops raw identifiers + raw payload content (but uses payload length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T16:01:28.496331Z",
     "iopub.status.busy": "2026-01-06T16:01:28.495384Z",
     "iopub.status.idle": "2026-01-06T16:01:28.897210Z",
     "shell.execute_reply": "2026-01-06T16:01:28.896404Z",
     "shell.execute_reply.started": "2026-01-06T16:01:28.496271Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_binary_labels(df: pd.DataFrame,\n",
    "                         label_col: str = LABEL_COLUMN,\n",
    "                         benign_label: str = BENIGN_LABEL_NAME) -> pd.DataFrame:\n",
    "    if label_col not in df.columns:\n",
    "        raise ValueError(f\"Label column '{label_col}' not found.\")\n",
    "\n",
    "    df = df.copy()\n",
    "    df[label_col] = df[label_col].astype(str)\n",
    "    df[\"binary_label\"] = (df[label_col] != benign_label).astype(int)\n",
    "\n",
    "    print(\"\\n[INFO] Original label distribution:\")\n",
    "    print(df[label_col].value_counts())\n",
    "    print(\"\\n[INFO] Binary label distribution (0=normal, 1=attack):\")\n",
    "    print(df[\"binary_label\"].value_counts())\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_feature_dataframe_full(df_raw: pd.DataFrame) -> tuple[pd.DataFrame, np.ndarray, list]:\n",
    "    \"\"\"\n",
    "    Final engineered feature set (tabular + interpretable):\n",
    "      - ports, ttl, traffic size\n",
    "      - log traffic size\n",
    "      - payload length (hex -> bytes)\n",
    "      - common destination port indicator\n",
    "      - hour of day\n",
    "      - protocol one-hot\n",
    "\n",
    "    Drops:\n",
    "      - srcip/dstip (identifiers)\n",
    "      - payload (raw high-dimensional)\n",
    "    \"\"\"\n",
    "    df = df_raw.copy()\n",
    "\n",
    "    # --- Payload length: hex string -> bytes (more realistic than string length) ---\n",
    "    if \"payload\" in df.columns:\n",
    "        payload_str = df[\"payload\"].fillna(\"\").astype(str)\n",
    "        df[\"payload_len\"] = (payload_str.str.len() // 2).astype(int)\n",
    "    else:\n",
    "        df[\"payload_len\"] = 0\n",
    "\n",
    "    # Numeric conversions\n",
    "    for col in [\"sport\", \"dsport\", \"sttl\", \"total_len\", \"stime\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    # Fix missing values lightly (more robust scaling later)\n",
    "    df[\"total_len\"] = df[\"total_len\"].fillna(0)\n",
    "    df[\"sport\"] = df[\"sport\"].fillna(0)\n",
    "    df[\"dsport\"] = df[\"dsport\"].fillna(0)\n",
    "    df[\"sttl\"] = df[\"sttl\"].fillna(0)\n",
    "\n",
    "    # Log transform of total_len (handle skew)\n",
    "    df[\"log_total_len\"] = np.log1p(df[\"total_len\"])\n",
    "\n",
    "    # Common destination ports flag\n",
    "    df[\"is_common_dport\"] = df[\"dsport\"].isin(EXPERIMENT_CONFIG[\"common_ports\"]).astype(int)\n",
    "\n",
    "    # Hour-of-day from UNIX timestamp\n",
    "    dt = pd.to_datetime(df[\"stime\"], unit=\"s\", errors=\"coerce\")\n",
    "    df[\"hour_of_day\"] = dt.dt.hour.fillna(0).astype(int)\n",
    "\n",
    "    # Drop identifier & raw payload\n",
    "    drop_cols = [c for c in [\"srcip\", \"dstip\", \"payload\"] if c in df.columns]\n",
    "    if drop_cols:\n",
    "        print(f\"\\n[INFO] Dropping ID/raw columns: {drop_cols}\")\n",
    "        df = df.drop(columns=drop_cols)\n",
    "\n",
    "    # One-hot protocol\n",
    "    if \"protocol_m\" not in df.columns:\n",
    "        raise ValueError(\"Expected 'protocol_m' in dataset.\")\n",
    "    df = pd.get_dummies(df, columns=[\"protocol_m\"], drop_first=True)\n",
    "\n",
    "    # Convert protocol dummy booleans to int for clarity\n",
    "    for c in df.columns:\n",
    "        if c.startswith(\"protocol_m_\"):\n",
    "            df[c] = df[c].astype(int)\n",
    "\n",
    "    # Feature list\n",
    "    numeric_cols = [\n",
    "        \"sport\", \"dsport\", \"sttl\", \"total_len\",\n",
    "        \"log_total_len\", \"payload_len\",\n",
    "        \"is_common_dport\", \"hour_of_day\"\n",
    "    ]\n",
    "    protocol_cols = [c for c in df.columns if c.startswith(\"protocol_m_\")]\n",
    "    feature_cols = numeric_cols + protocol_cols\n",
    "\n",
    "    X_features = df[feature_cols].copy()\n",
    "    y = df[\"binary_label\"].values\n",
    "\n",
    "    print(\"\\n[INFO] Feature columns used:\")\n",
    "    print(feature_cols)\n",
    "    print(\"[INFO] Feature matrix shape:\", X_features.shape)\n",
    "\n",
    "    return X_features, y, feature_cols\n",
    "\n",
    "\n",
    "df = create_binary_labels(df, label_col=LABEL_COLUMN, benign_label=BENIGN_LABEL_NAME)\n",
    "X_all_features, y_all, feature_cols = build_feature_dataframe_full(df)\n",
    "\n",
    "print(\"\\n[INFO] Sample engineered features:\")\n",
    "display(X_all_features.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splits data (stratified), cleans numeric issues, and scales features.\n",
    "## Scaler is fit on TRAIN only to prevent leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T16:01:32.894595Z",
     "iopub.status.busy": "2026-01-06T16:01:32.894252Z",
     "iopub.status.idle": "2026-01-06T16:01:33.079823Z",
     "shell.execute_reply": "2026-01-06T16:01:33.079017Z",
     "shell.execute_reply.started": "2026-01-06T16:01:32.894572Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def summarize_split(name, y):\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    print(f\"\\n--- {name} Split ---\")\n",
    "    print(\"Total samples:\", len(y))\n",
    "    print(\"Label counts (0=normal,1=attack):\", dict(zip(unique, counts)))\n",
    "\n",
    "\n",
    "def clean_features(df_features: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_features.apply(pd.to_numeric, errors=\"coerce\")\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "\n",
    "def scale_features(train_df, val_df, test_df, feature_cols):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(train_df[feature_cols].values)\n",
    "    X_val_scaled = scaler.transform(val_df[feature_cols].values)\n",
    "    X_test_scaled = scaler.transform(test_df[feature_cols].values)\n",
    "    return X_train_scaled, X_val_scaled, X_test_scaled, scaler\n",
    "\n",
    "\n",
    "# Split\n",
    "X_temp, X_test_df, y_temp, y_test = train_test_split(\n",
    "    X_all_features, y_all,\n",
    "    test_size=EXPERIMENT_CONFIG[\"test_size\"],\n",
    "    stratify=y_all,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "X_train_df, X_val_df, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=EXPERIMENT_CONFIG[\"val_size\"],\n",
    "    stratify=y_temp,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "summarize_split(\"Train\", y_train)\n",
    "summarize_split(\"Validation\", y_val)\n",
    "summarize_split(\"Test\", y_test)\n",
    "\n",
    "# Clean\n",
    "X_train_df = clean_features(X_train_df)\n",
    "X_val_df = clean_features(X_val_df)\n",
    "X_test_df = clean_features(X_test_df)\n",
    "\n",
    "# Scale\n",
    "X_train_scaled, X_val_scaled, X_test_scaled, scaler = scale_features(\n",
    "    X_train_df, X_val_df, X_test_df, feature_cols\n",
    ")\n",
    "\n",
    "print(\"\\n[INFO] Shapes after scaling:\")\n",
    "print(\"X_train_scaled:\", X_train_scaled.shape)\n",
    "print(\"X_val_scaled  :\", X_val_scaled.shape)\n",
    "print(\"X_test_scaled :\", X_test_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusable helpers: metrics, confusion matrix printing, PR curve plotting, threshold tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T16:01:36.024538Z",
     "iopub.status.busy": "2026-01-06T16:01:36.024184Z",
     "iopub.status.idle": "2026-01-06T16:01:36.038075Z",
     "shell.execute_reply": "2026-01-06T16:01:36.036882Z",
     "shell.execute_reply.started": "2026-01-06T16:01:36.024518Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, scores, model_name: str) -> dict:\n",
    "    metrics = {\n",
    "        \"model\": model_name,\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"balanced_accuracy\": balanced_accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"mcc\": matthews_corrcoef(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "    if scores is not None:\n",
    "        try:\n",
    "            metrics[\"roc_auc\"] = roc_auc_score(y_true, scores)\n",
    "        except ValueError:\n",
    "            metrics[\"roc_auc\"] = np.nan\n",
    "\n",
    "        prec, rec, _ = precision_recall_curve(y_true, scores)\n",
    "        metrics[\"pr_auc\"] = auc(rec, prec)\n",
    "    else:\n",
    "        metrics[\"roc_auc\"] = np.nan\n",
    "        metrics[\"pr_auc\"] = np.nan\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def print_confusion_details(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(f\"TN (correct normals) : {tn}\")\n",
    "    print(f\"FP (false alarms)    : {fp}\")\n",
    "    print(f\"FN (missed attacks)  : {fn}\")\n",
    "    print(f\"TP (detected attacks): {tp}\")\n",
    "\n",
    "\n",
    "def print_metrics(metrics: dict):\n",
    "    print(f\"\\n=== {metrics['model']} ===\")\n",
    "    print(f\"Accuracy          : {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Balanced Accuracy : {metrics['balanced_accuracy']:.4f}\")\n",
    "    print(f\"Precision         : {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall            : {metrics['recall']:.4f}\")\n",
    "    print(f\"F1-score          : {metrics['f1']:.4f}\")\n",
    "    print(f\"MCC               : {metrics['mcc']:.4f}\")\n",
    "    print(\"ROC-AUC           : \", \"N/A\" if np.isnan(metrics['roc_auc']) else f\"{metrics['roc_auc']:.4f}\")\n",
    "    print(\"PR-AUC            : \", \"N/A\" if np.isnan(metrics['pr_auc']) else f\"{metrics['pr_auc']:.4f}\")\n",
    "\n",
    "\n",
    "def plot_precision_recall(scores_dict: dict, y_true, title_suffix=\"\"):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for model_name, scores in scores_dict.items():\n",
    "        prec, rec, _ = precision_recall_curve(y_true, scores)\n",
    "        pr_auc_val = auc(rec, prec)\n",
    "        plt.plot(rec, prec, label=f\"{model_name} (PR-AUC={pr_auc_val:.3f})\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(f\"Precision–Recall Curves {title_suffix}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def tune_threshold_max_f1(y_val, val_scores):\n",
    "    \"\"\"\n",
    "    Tune threshold on validation scores by maximizing F1.\n",
    "    Important: use precision[:-1] / recall[:-1] to align with thresholds.\n",
    "    \"\"\"\n",
    "    prec, rec, thresh = precision_recall_curve(y_val, val_scores)\n",
    "    f1s = (2 * prec[:-1] * rec[:-1]) / (prec[:-1] + rec[:-1] + 1e-12)\n",
    "    best_idx = int(np.argmax(f1s))\n",
    "    best_thr = float(thresh[best_idx])\n",
    "    best_f1 = float(f1s[best_idx])\n",
    "    return best_thr, best_f1\n",
    "\n",
    "print(\"Evaluation helpers ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolation Forest learns “normal” from benign training samples.\n",
    "## We tune the decision threshold on validation by max F1, then evaluate on test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T16:11:06.533813Z",
     "iopub.status.busy": "2026-01-06T16:11:06.533455Z",
     "iopub.status.idle": "2026-01-06T16:11:10.677787Z",
     "shell.execute_reply": "2026-01-06T16:11:10.677178Z",
     "shell.execute_reply.started": "2026-01-06T16:11:06.533793Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train_benign = X_train_scaled[y_train == 0]\n",
    "print(f\"[IF] Training on benign-only train subset: {X_train_benign.shape[0]} samples\")\n",
    "\n",
    "iso = IsolationForest(\n",
    "    contamination=(0.01),\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "iso.fit(X_train_benign)\n",
    "\n",
    "val_scores_if = -iso.decision_function(X_val_scaled)\n",
    "test_scores_if = -iso.decision_function(X_test_scaled)\n",
    "\n",
    "best_thresh_if, best_f1_if = tune_threshold_max_f1(y_val, val_scores_if)\n",
    "print(f\"[IF] Best validation F1={best_f1_if:.4f} at threshold={best_thresh_if:.6f}\")\n",
    "\n",
    "y_test_pred_if = (test_scores_if >= best_thresh_if).astype(int)\n",
    "metrics_if = evaluate_model(y_test, y_test_pred_if, test_scores_if, \"IsolationForest (benign-only, tuned)\")\n",
    "print_metrics(metrics_if)\n",
    "print_confusion_details(y_test, y_test_pred_if)\n",
    "plot_precision_recall({\"IsolationForest\": test_scores_if}, y_test, title_suffix=\"(IsolationForest)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOF detects anomalies as points with lower local density than neighbors.\n",
    "## We fit on benign-only and score validation/test flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T17:58:18.443222Z",
     "iopub.status.busy": "2026-01-05T17:58:18.442487Z",
     "iopub.status.idle": "2026-01-05T17:58:24.242058Z",
     "shell.execute_reply": "2026-01-05T17:58:24.241332Z",
     "shell.execute_reply.started": "2026-01-05T17:58:18.443197Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"[LOF] Training on benign-only train subset: {X_train_benign.shape[0]} samples\")\n",
    "\n",
    "lof = LocalOutlierFactor(\n",
    "    n_neighbors=20,\n",
    "    novelty=True,\n",
    "    n_jobs=-1\n",
    ")\n",
    "lof.fit(X_train_benign)\n",
    "\n",
    "val_scores_lof = -lof.score_samples(X_val_scaled)\n",
    "test_scores_lof = -lof.score_samples(X_test_scaled)\n",
    "\n",
    "best_thresh_lof, best_f1_lof = tune_threshold_max_f1(y_val, val_scores_lof)\n",
    "print(f\"[LOF] Best validation F1={best_f1_lof:.4f} at threshold={best_thresh_lof:.6f}\")\n",
    "\n",
    "y_test_pred_lof = (test_scores_lof >= best_thresh_lof).astype(int)\n",
    "metrics_lof = evaluate_model(y_test, y_test_pred_lof, test_scores_lof, \"LOF (benign-only, tuned)\")\n",
    "print_metrics(metrics_lof)\n",
    "print_confusion_details(y_test, y_test_pred_lof)\n",
    "plot_precision_recall({\"LOF\": test_scores_lof}, y_test, title_suffix=\"(LOF)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder reconstructs normal traffic with low error.\n",
    "## High reconstruction error indicates anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T18:18:13.864754Z",
     "iopub.status.busy": "2026-01-05T18:18:13.864078Z",
     "iopub.status.idle": "2026-01-05T18:18:13.873413Z",
     "shell.execute_reply": "2026-01-05T18:18:13.872484Z",
     "shell.execute_reply.started": "2026-01-05T18:18:13.864727Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "metrics_ae = None\n",
    "\n",
    "if not KERAS_AVAILABLE:\n",
    "    print(\"[AE] TensorFlow/Keras not available; skipping Autoencoder.\")\n",
    "else:\n",
    "    print(\"[AE] Training Autoencoder on benign-only train subset.\")\n",
    "\n",
    "    input_dim = X_train_scaled.shape[1]\n",
    "    ae_epochs = EXPERIMENT_CONFIG[\"ae_epochs\"]\n",
    "    ae_batch_size = EXPERIMENT_CONFIG[\"ae_batch_size\"]\n",
    "\n",
    "    inp = Input(shape=(input_dim,))\n",
    "    x = Dense(16, activation=\"relu\")(inp)\n",
    "    x = Dense(8, activation=\"relu\")(x)\n",
    "    x = Dense(16, activation=\"relu\")(x)\n",
    "    out = Dense(input_dim, activation=\"linear\")(x)\n",
    "\n",
    "    autoencoder = Model(inputs=inp, outputs=out)\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=1e-3), loss=\"mse\")\n",
    "\n",
    "    autoencoder.fit(\n",
    "        X_train_benign, X_train_benign,\n",
    "        epochs=ae_epochs,\n",
    "        batch_size=ae_batch_size,\n",
    "        shuffle=True,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Scores = reconstruction error\n",
    "    val_recon = autoencoder.predict(X_val_scaled, verbose=0)\n",
    "    val_mse = np.mean((X_val_scaled - val_recon) ** 2, axis=1)\n",
    "\n",
    "    test_recon = autoencoder.predict(X_test_scaled, verbose=0)\n",
    "    test_mse = np.mean((X_test_scaled - test_recon) ** 2, axis=1)\n",
    "\n",
    "    best_thresh_ae, best_f1_ae = tune_threshold_max_f1(y_val, val_mse)\n",
    "    print(f\"[AE] Best validation F1={best_f1_ae:.4f} at threshold={best_thresh_ae:.6f}\")\n",
    "\n",
    "    y_test_pred_ae = (test_mse >= best_thresh_ae).astype(int)\n",
    "    metrics_ae = evaluate_model(y_test, y_test_pred_ae, test_mse, \"Autoencoder (benign-only, tuned)\")\n",
    "    print_metrics(metrics_ae)\n",
    "    print_confusion_details(y_test, y_test_pred_ae)\n",
    "    plot_precision_recall({\"Autoencoder\": test_mse}, y_test, title_suffix=\"(Autoencoder)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T18:19:03.645482Z",
     "iopub.status.busy": "2026-01-05T18:19:03.644849Z",
     "iopub.status.idle": "2026-01-05T18:19:30.519513Z",
     "shell.execute_reply": "2026-01-05T18:19:30.518725Z",
     "shell.execute_reply.started": "2026-01-05T18:19:03.645458Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "n_db = min(EXPERIMENT_CONFIG[\"max_dbscan_samples\"], X_test_scaled.shape[0])\n",
    "\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "db_idx = rng.choice(len(X_test_scaled), size=n_db, replace=False)\n",
    "\n",
    "X_db = X_test_scaled[db_idx]\n",
    "y_db_true = y_test[db_idx]\n",
    "\n",
    "print(\"[DBSCAN] Fitting PCA on benign-only training data...\")\n",
    "\n",
    "pca = PCA(\n",
    "    n_components=EXPERIMENT_CONFIG[\"pca_components_dbscan\"],\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "X_train_benign = X_train_scaled[y_train == 0]\n",
    "X_train_benign_pca = pca.fit_transform(X_train_benign)\n",
    "\n",
    "print(\"[DBSCAN] Fitting DBSCAN on benign-only training data...\")\n",
    "\n",
    "dbscan = DBSCAN(\n",
    "    eps=EXPERIMENT_CONFIG[\"dbscan_eps\"],\n",
    "    min_samples=EXPERIMENT_CONFIG[\"dbscan_min_samples\"],\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "dbscan.fit(X_train_benign_pca)\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "print(\"[DBSCAN] Classifying test samples by distance to DBSCAN core points...\")\n",
    "\n",
    "core_samples = dbscan.components_\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=1)\n",
    "nn.fit(core_samples)\n",
    "\n",
    "X_db_pca = pca.transform(X_db)\n",
    "\n",
    "distances, _ = nn.kneighbors(X_db_pca)\n",
    "\n",
    "y_pred_db = (distances[:, 0] > EXPERIMENT_CONFIG[\"dbscan_eps\"]).astype(int)\n",
    "\n",
    "metrics_db = evaluate_model(y_db_true, y_pred_db, None, \"DBSCAN+PCA (inductive baseline)\")\n",
    "print_metrics(metrics_db)\n",
    "print_confusion_details(y_db_true, y_pred_db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T17:58:32.831307Z",
     "iopub.status.busy": "2026-01-05T17:58:32.830558Z",
     "iopub.status.idle": "2026-01-05T17:58:36.276389Z",
     "shell.execute_reply": "2026-01-05T17:58:36.275366Z",
     "shell.execute_reply.started": "2026-01-05T17:58:32.831282Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- DBSCAN + PCA (exploratory baseline, leakage-safe, unbiased sampling) ---\n",
    "\n",
    "n_db = min(EXPERIMENT_CONFIG[\"max_dbscan_samples\"], X_test_scaled.shape[0])\n",
    "\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "db_idx = rng.choice(len(X_test_scaled), size=n_db, replace=False)\n",
    "\n",
    "X_db = X_test_scaled[db_idx]\n",
    "y_db_true = y_test[db_idx]\n",
    "\n",
    "print(\"[DBSCAN] Fitting PCA on benign-only training data...\")\n",
    "pca = PCA(n_components=EXPERIMENT_CONFIG[\"pca_components_dbscan\"], random_state=RANDOM_SEED)\n",
    "pca.fit(X_train_scaled[y_train == 0])\n",
    "\n",
    "X_db_pca = pca.transform(X_db)\n",
    "print(f\"[DBSCAN] PCA-transformed test shape: {X_db_pca.shape}\")\n",
    "\n",
    "print(f\"[DBSCAN] Running DBSCAN on {n_db} TEST samples...\")\n",
    "dbscan = DBSCAN(\n",
    "    eps=EXPERIMENT_CONFIG[\"dbscan_eps\"],\n",
    "    min_samples=EXPERIMENT_CONFIG[\"dbscan_min_samples\"],\n",
    "    n_jobs=-1\n",
    ")\n",
    "labels_db = dbscan.fit_predict(X_db_pca)\n",
    "y_pred_db = (labels_db == -1).astype(int)\n",
    "\n",
    "metrics_db = evaluate_model(y_db_true, y_pred_db, None, \"DBSCAN+PCA (exploratory baseline)\")\n",
    "print_metrics(metrics_db)\n",
    "print_confusion_details(y_db_true, y_pred_db)\n",
    "\n",
    "unique_labels, counts = np.unique(labels_db, return_counts=True)\n",
    "print(\"\\n[DBSCAN] Cluster distribution (including noise = -1):\")\n",
    "print(dict(zip(unique_labels, counts)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The final result table and saves it for the thesis appendix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T17:59:04.980720Z",
     "iopub.status.busy": "2026-01-05T17:59:04.980069Z",
     "iopub.status.idle": "2026-01-05T17:59:04.997149Z",
     "shell.execute_reply": "2026-01-05T17:59:04.996281Z",
     "shell.execute_reply.started": "2026-01-05T17:59:04.980681Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "all_metrics = [metrics_if, metrics_lof, metrics_db]\n",
    "if metrics_ae is not None:\n",
    "    all_metrics.append(metrics_ae)\n",
    "\n",
    "metrics_df = pd.DataFrame(all_metrics)\n",
    "print(\"\\n=== Combined Metrics Summary (Test Set) ===\")\n",
    "display(metrics_df)\n",
    "\n",
    "out_path = \"aci_iot_results_thesis_final.csv\"\n",
    "metrics_df.to_csv(out_path, index=False)\n",
    "print(f\"\\n[INFO] Saved combined results to {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shows how Precision/Recall/F1 vary with threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T17:59:17.744078Z",
     "iopub.status.busy": "2026-01-05T17:59:17.743382Z",
     "iopub.status.idle": "2026-01-05T17:59:18.949401Z",
     "shell.execute_reply": "2026-01-05T17:59:18.948445Z",
     "shell.execute_reply.started": "2026-01-05T17:59:17.744054Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def threshold_tradeoff_plot(y_true, scores, chosen_threshold=None, title=\"Threshold trade-off\"):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, scores)\n",
    "    precision_t = precision[:-1]\n",
    "    recall_t = recall[:-1]\n",
    "\n",
    "    f1 = (2 * precision_t * recall_t) / (precision_t + recall_t + 1e-12)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(thresholds, precision_t, label=\"Precision\")\n",
    "    plt.plot(thresholds, recall_t, label=\"Recall\")\n",
    "    plt.plot(thresholds, f1, label=\"F1\")\n",
    "\n",
    "    if chosen_threshold is not None:\n",
    "        plt.axvline(chosen_threshold, linestyle=\"--\", linewidth=2, label=f\"Chosen threshold={chosen_threshold:.4f}\")\n",
    "\n",
    "    plt.xlabel(\"Threshold (score ≥ threshold ⇒ anomaly)\")\n",
    "    plt.ylabel(\"Metric\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def pr_curve_with_threshold_marker(y_true, scores, chosen_threshold=None, title=\"Precision–Recall Curve\"):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, scores)\n",
    "\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.plot(recall, precision, label=\"PR curve\")\n",
    "\n",
    "    if chosen_threshold is not None and len(thresholds) > 0:\n",
    "        idx = np.argmin(np.abs(thresholds - chosen_threshold))\n",
    "        plt.scatter([recall[idx]], [precision[idx]], s=80, label=f\"Operating point @ {chosen_threshold:.4f}\")\n",
    "\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# IF\n",
    "threshold_tradeoff_plot(y_val, val_scores_if, best_thresh_if, \"Isolation Forest — Threshold Trade-off (Validation)\")\n",
    "pr_curve_with_threshold_marker(y_val, val_scores_if, best_thresh_if, \"Isolation Forest — PR Curve (Validation)\")\n",
    "\n",
    "# LOF\n",
    "threshold_tradeoff_plot(y_val, val_scores_lof, best_thresh_lof, \"LOF — Threshold Trade-off (Validation)\")\n",
    "pr_curve_with_threshold_marker(y_val, val_scores_lof, best_thresh_lof, \"LOF — PR Curve (Validation)\")\n",
    "\n",
    "# AE (if present)\n",
    "if KERAS_AVAILABLE and \"val_mse\" in globals():\n",
    "    threshold_tradeoff_plot(y_val, val_mse, best_thresh_ae, \"Autoencoder — Threshold Trade-off (Validation)\")\n",
    "    pr_curve_with_threshold_marker(y_val, val_mse, best_thresh_ae, \"Autoencoder — PR Curve (Validation)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shows why threshold tuning is essential in unsupervised anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T17:59:23.817970Z",
     "iopub.status.busy": "2026-01-05T17:59:23.817667Z",
     "iopub.status.idle": "2026-01-05T17:59:25.241674Z",
     "shell.execute_reply": "2026-01-05T17:59:25.240772Z",
     "shell.execute_reply.started": "2026-01-05T17:59:23.817947Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "labels_default = iso.predict(X_test_scaled)  # 1 normal, -1 anomaly\n",
    "y_test_pred_default_if = (labels_default == -1).astype(int)\n",
    "\n",
    "metrics_if_default = evaluate_model(y_test, y_test_pred_default_if, test_scores_if, \"IF (default threshold)\")\n",
    "metrics_if_tuned = evaluate_model(y_test, y_test_pred_if, test_scores_if, \"IF (tuned threshold)\")\n",
    "\n",
    "print(\"\\n=== Ablation 1: Isolation Forest — Default vs Tuned (Test set) ===\")\n",
    "display(pd.DataFrame([metrics_if_default, metrics_if_tuned]))\n",
    "\n",
    "print(\"\\n[Default threshold] classification report:\")\n",
    "print(classification_report(y_test, y_test_pred_default_if, digits=4, zero_division=0))\n",
    "\n",
    "print(\"\\n[Tuned threshold] classification report:\")\n",
    "print(classification_report(y_test, y_test_pred_if, digits=4, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantifies how feature engineering affects anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T17:59:26.188019Z",
     "iopub.status.busy": "2026-01-05T17:59:26.187297Z",
     "iopub.status.idle": "2026-01-05T17:59:31.099238Z",
     "shell.execute_reply": "2026-01-05T17:59:31.098657Z",
     "shell.execute_reply.started": "2026-01-05T17:59:26.187992Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_basic_features(df_raw):\n",
    "    df = df_raw.copy()\n",
    "    drop_cols = [c for c in [\"srcip\", \"dstip\", \"payload\"] if c in df.columns]\n",
    "    df = df.drop(columns=drop_cols, errors=\"ignore\")\n",
    "\n",
    "    for col in [\"sport\", \"dsport\", \"sttl\", \"total_len\", \"stime\"]:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0)\n",
    "\n",
    "    df = pd.get_dummies(df, columns=[\"protocol_m\"], drop_first=True)\n",
    "    for c in df.columns:\n",
    "        if c.startswith(\"protocol_m_\"):\n",
    "            df[c] = df[c].astype(int)\n",
    "\n",
    "    numeric_cols = [\"sport\", \"dsport\", \"sttl\", \"total_len\", \"stime\"]\n",
    "    protocol_cols = [c for c in df.columns if c.startswith(\"protocol_m_\")]\n",
    "    cols = numeric_cols + protocol_cols\n",
    "    return df[cols], cols\n",
    "\n",
    "\n",
    "def run_if_pipeline(X_features_df, y, cols, name_prefix=\"\"):\n",
    "    X_temp, X_test_l, y_temp, y_test_l = train_test_split(\n",
    "        X_features_df, y,\n",
    "        test_size=EXPERIMENT_CONFIG[\"test_size\"],\n",
    "        stratify=y,\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    X_train_l, X_val_l, y_train_l, y_val_l = train_test_split(\n",
    "        X_temp, y_temp,\n",
    "        test_size=EXPERIMENT_CONFIG[\"val_size\"],\n",
    "        stratify=y_temp,\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = scaler.fit_transform(X_train_l[cols].values)\n",
    "    X_val_s = scaler.transform(X_val_l[cols].values)\n",
    "    X_test_s = scaler.transform(X_test_l[cols].values)\n",
    "\n",
    "    X_train_b = X_train_s[y_train_l == 0]\n",
    "\n",
    "    model = IsolationForest(contamination=\"auto\", random_state=RANDOM_SEED, n_jobs=-1)\n",
    "    model.fit(X_train_b)\n",
    "\n",
    "    val_scores = -model.decision_function(X_val_s)\n",
    "    test_scores = -model.decision_function(X_test_s)\n",
    "\n",
    "    thr, f1v = tune_threshold_max_f1(y_val_l, val_scores)\n",
    "    y_pred = (test_scores >= thr).astype(int)\n",
    "\n",
    "    return evaluate_model(y_test_l, y_pred, test_scores, f\"{name_prefix} IF (tuned)\")\n",
    "\n",
    "\n",
    "# Basic\n",
    "X_basic, basic_cols = build_basic_features(df)\n",
    "metrics_basic = run_if_pipeline(X_basic, df[\"binary_label\"].values, basic_cols, name_prefix=\"Basic\")\n",
    "\n",
    "# Engineered (your final pipeline)\n",
    "metrics_eng = run_if_pipeline(X_all_features, df[\"binary_label\"].values, feature_cols, name_prefix=\"Engineered\")\n",
    "\n",
    "print(\"\\n=== Ablation 2: Feature Engineering Impact (Isolation Forest) ===\")\n",
    "display(pd.DataFrame([metrics_basic, metrics_eng]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prints a short justification for your metric choices under class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T17:59:34.693359Z",
     "iopub.status.busy": "2026-01-05T17:59:34.692810Z",
     "iopub.status.idle": "2026-01-05T17:59:34.700384Z",
     "shell.execute_reply": "2026-01-05T17:59:34.699567Z",
     "shell.execute_reply.started": "2026-01-05T17:59:34.693334Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "positive_rate = float(np.mean(y_test))\n",
    "\n",
    "print(\"=== Metric Choice Justification (Test Set) ===\")\n",
    "print(f\"Attack rate (positive class proportion): {positive_rate:.4f}\")\n",
    "\n",
    "print(\"\\nWhy Accuracy is not enough:\")\n",
    "print(\"- With heavy class imbalance, predicting 'normal' most of the time can still produce high accuracy.\")\n",
    "\n",
    "print(\"\\nWhy Precision/Recall/F1:\")\n",
    "print(\"- Precision measures false alarm burden (how many alerts are truly attacks).\")\n",
    "print(\"- Recall measures detection rate (how many attacks are detected).\")\n",
    "print(\"- F1 summarizes the trade-off at a chosen operating threshold.\")\n",
    "\n",
    "print(\"\\nWhy ROC-AUC and PR-AUC:\")\n",
    "print(\"- ROC-AUC evaluates ranking quality independent of threshold.\")\n",
    "print(\"- PR-AUC is more informative than ROC-AUC under class imbalance (focus on the positive class).\")\n",
    "print(f\"- Baseline PR-AUC for a random ranker ≈ attack rate ≈ {positive_rate:.4f}\")\n",
    "\n",
    "print(\"\\nWhy Balanced Accuracy and MCC:\")\n",
    "print(\"- Balanced accuracy treats both classes equally (mean of TPR and TNR).\")\n",
    "print(\"- MCC remains informative under imbalance and penalizes both FP and FN.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4256269,
     "sourceId": 8215684,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
